{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Data Collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![steam_logo](https://buy.thewitcher.com/img/shops/steam.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast-forward a little over 10 years and the <a href=\"https://en.wikipedia.org/wiki/Steam_(software)\">Steam Store</a> is huge, ubiquitous as the home of PC gaming and distribution. Whilst physical copies still just about feel at home on consoles, the PC market has [long since moved digital](https://www.pcgamer.com/uk/analyst-says-digital-sales-made-up-92-percent-of-pc-game-market-in-2013/). In case you are not familiar, Steam is a digital store for purchasing, downloading and playing video games. It hosts a variety of community features, allows pushing game updates to users automatically, and gathers news stories relevant to each title. It's a bit like [Google's Play Store](https://play.google.com) or [Apple's App Store](https://www.apple.com/uk/ios/app-store/) for phones.\n",
    "\n",
    "A large part of Steam's success as a platform is due to its use of frequent sales, convenience as a unified digital game library, and the aforementioned shift to digital over physical. Whilst other platforms are emerging and gaining traction, there is likely no better resource for examining gaming over the last decade. With that in mind, if we can construct a dataset from Steam's data, we will have access to a wealth of information of more than [ 30,000 games](https://store.steampowered.com/about/) released since 2003, when Steam first launched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goals\n",
    "\n",
    "<!-- PELICAN_BEGIN_SUMMARY -->\n",
    "\n",
    "The motivation for this project is to download, process and analyse a data set of Steam apps (games) from the Steam store, and gain insights into what makes a game more successful in terms of sales, play-time and ratings. We will imagine that we have been approached by a company hoping to develop and release a new title, using the findings we provide them to inform decisions about how best to manage their budget and hopefully increase the success of their next release.\n",
    "\n",
    "The first step will be tackling data collection - the actual retrieval of data from Steam's servers and databases. In the future we'll look at cleaning the data, transforming it into a more useful state, then on to data exploration and analysis. Finally we'll summarise our findings in a non-technical report which would be sent to the fictional company in question.\n",
    "\n",
    "<!-- PELICAN_END_SUMMARY -->\n",
    "\n",
    "At the end of the data collection and cleaning stages, we'd like to end up with a table or database like this:\n",
    "\n",
    "name | id | information | owners | price | rating\n",
    "--- | --- | --- | --- | --- | ---\n",
    "awesome game | 100 | genres, descriptors, variables | 100,000 | 9.99 .- | 9/10\n",
    "generic shooter 4 | 200 | just a regular shooter, teabag option available | 50,000 | 39.99 .- | 6/10\n",
    "... | ... | ... | ... | ... | ...\n",
    "\n",
    "We can then interrogate the data, and investigate whether particular attributes tend to result in more successful games. Metrics like ownership and ratings should help define the success of a title.\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "There are a number of ways to get this information. Obviously we could search the web (and especially [kaggle](https://www.kaggle.com/datasets)) for existing datasets, however to avoid letting someone else get away with all that hard work (and mainly for the purposes of learning) we'll be acquiring all the data ourselves from scratch.\n",
    "\n",
    "Often when generating data the best place to start is to check for APIs. Fortunately Valve (the company behind Steam) make one available at https://partner.steamgames.com/. An [API](https://en.wikipedia.org/wiki/Application_programming_interface) such as this allows anyone to interface with data on a website in a controlled way, usually providing a host of useful features to the end-user. Typically an API is a great way for developers to allow access to databases and information on a server. Unfortunately this documentation doesn't include all access points, but others have documented this for us. [This](https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI) documentation of the StorefrontAPI will be particularly useful.\n",
    "\n",
    "We'll be able to get good information about the details of each game from the Steam API, however we're still missing information about popularity and sales. Luckily we can easily get this data from another website, SteamSpy.\n",
    "\n",
    "[SteamSpy](https://steamspy.com/about) is a Steam stats-gathering service and crucially has data easily available through its own API (documentation [here](https://steamspy.com/api.php)). It provides a number of useful metrics including an estimation for total owners of each game.\n",
    "\n",
    "We'll be retrieving data from both APIs and combining them to form our dataset. For the purposes of this project, we'll be performing as little data cleaning as possible at this stage, providing 'dirty' data for data cleaning, the next step in this project.\n",
    "\n",
    "## Section outline:\n",
    "\n",
    "- Create an app list from SteamSpy API using 'all' request\n",
    "- Retrieve individual app data from Steam API, by iterating through app list\n",
    "- Retrieve individual app data from SteamSpy API, by iterating through app list\n",
    "- Export app list, Steam data and SteamSpy data to csv files\n",
    "\n",
    "## API references:\n",
    "\n",
    "- https://partner.steamgames.com/doc/webapi\n",
    "- https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI\n",
    "- https://steamapi.xpaw.me/#\n",
    "- https://steamspy.com/api.php\n",
    "\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "We begin by importing the libraries we will be using. We start with [standard library imports](https://docs.python.org/3/library/), or those available by default in Python, then import the third-party packages. We'll be using [requests](https://2.python-requests.org/en/master/) to handle interacting with the APIs, then the popular [pandas](http://pandas.pydata.org/pandas-docs/stable/) and [numpy](https://docs.scipy.org/doc/numpy/index.html) libraries for handling the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import csv\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Number of game processed (In our case, we wanted to parse all the games [-1 indicates this case])\n",
    "end = -1\n",
    "\n",
    "# Identifier to differentiate the different versions of the original data\n",
    "data_id='1'\n",
    "\n",
    "# customisations - ensure tables show all columns\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a general, all-purpose function to process get requests from an API, supplied through a URL parameter. A dictionary of parameters can be supplied which is passed into the get request automatically, depending on the requirements of the API.\n",
    "\n",
    "Rather than simply returning the response, we handle a couple of scenarios to help automation. Occasionally we encounter an SSL Error, in which case we simply wait a few seconds then try again (by recursively calling the function). When this happens, and generally throughout this project, we provide quite verbose feedback to show when these errors are encountered and how they are handled.\n",
    "\n",
    "Sometimes there is no response when a request is made (returns None). This usually happens when too many requests are made in a short period of time, and the polling limit has been reached. We try to avoid this by pausing briefly between requests, as we'll see later, but in case we breach the polling limit we wait 10 seconds then try again.\n",
    "\n",
    "Handling these errors in this way ensures that our function almost always returns the desired response, which we return in json format to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url, parameters=None):\n",
    "    \"\"\"Return json-formatted response of a get request using optional parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "        parameters to pass as part of get request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url=url, params=parameters)\n",
    "    except SSLError as s:\n",
    "        print('SSL Error:', s)\n",
    "        \n",
    "        for i in range(5, 0, -1):\n",
    "            print('\\rWaiting... ({})'.format(i), end='')\n",
    "            time.sleep(1)\n",
    "        print('\\rRetrying.' + ' '*10)\n",
    "        \n",
    "        # recusively try again\n",
    "        return get_request(url, parameters)\n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # response is none usually means too many requests. Wait and try again \n",
    "        print('No response, waiting 10 seconds...')\n",
    "        time.sleep(10)\n",
    "        print('Retrying.')\n",
    "        return get_request(url, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate List of App IDs\n",
    "\n",
    "Every app on the steam store has a unique app ID. Whilst different apps can have the same name, they can't have the same ID. This will be very useful to us for identifying apps and eventually merging our tables of data.\n",
    "\n",
    "Before we get to that, we need to generate a list of app ids which we can use to build our data sets. It's possible to generate one from the Steam API, however this has over 70,000 entries, many of which are demos and videos with no way to tell them apart. Instead, SteamSpy provides an 'all' request, supplying some information about the apps they track. It doesn't supply all information about each app, so we still need to request this information individually, but it provides a good starting point.\n",
    "\n",
    "Because many of the return fields are strings containing commas and other punctuation, it is easiest to read the response into a pandas dataframe, and export the required appid and name fields to a csv. We could keep only the appid column as a list or pandas series, but it may be useful to keep the app name at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33099"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://steamspy.com/api.php\"\n",
    "parameters = {\"request\": \"all\"}\n",
    "\n",
    "# request 'all' from steam spy and parse into dataframe\n",
    "json_data = get_request(url, parameters=parameters)\n",
    "steam_spy_all = pd.DataFrame.from_dict(json_data, orient='index')\n",
    "\n",
    "# generate sorted app_list from steamspy data\n",
    "app_list = steam_spy_all[['appid', 'name']].sort_values('appid').reset_index(drop=True)\n",
    "\n",
    "# export disabled to keep consistency across download sessions\n",
    "app_list.to_csv('./data/download/app_list'+data_id+'.csv', index=False)    # If you want to start parsing the steam database by yourself, just uncomment this line\n",
    "\n",
    "# instead read from stored csv\n",
    "app_list = pd.read_csv('./data/download/app_list'+data_id+'.csv')\n",
    "\n",
    "# display first few rows\n",
    "app_list.head()\n",
    "len(app_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Download Logic\n",
    "\n",
    "Now we have the `app_list` dataframe, we can iterate over the app IDs and request individual app data from the servers. Here we set out our logic to retrieve and process this information, then finally store the data as a csv file.\n",
    "\n",
    "Because it takes a long time to retrieve the data, it would be dangerous to attempt it all in one go as any errors or connection time-outs could cause the loss of all our data. For this reason we define a function to download and process the requests in batches, appending each batch to an external file and keeping track of the highest index written in a separate file.\n",
    "\n",
    "This not only provides security, allowing us to easily restart the process if an error is encountered, but also means we can complete the download across multiple sessions.\n",
    "\n",
    "Again, we provide verbose output for rows exported, batches complete, time taken and estimated time remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_data(start, stop, parser, pause):\n",
    "    \"\"\"Return list of app data generated from parser.\n",
    "    \n",
    "    parser : function to handle request\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "    \n",
    "    # iterate through each row of app_list, confined by start and stop\n",
    "    for index, row in app_list[start:stop].iterrows():\n",
    "        print('Current index: {}'.format(index), end='\\r')\n",
    "        \n",
    "        appid = row['appid']\n",
    "        name = row['name']\n",
    "\n",
    "        # retrive app data for a row, handled by supplied parser, and append to list\n",
    "        data = parser(appid, name)\n",
    "        app_data.append(data)\n",
    "\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "    \n",
    "    return app_data\n",
    "\n",
    "\n",
    "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
    "                    columns, begin=0, end=-1, batchsize=100, pause=1):\n",
    "    \"\"\"Process app data in batches, writing directly to file.\n",
    "    \n",
    "    parser : custom function to format request\n",
    "    app_list : dataframe of appid and name\n",
    "    download_path : path to store data\n",
    "    data_filename : filename to save app data\n",
    "    index_filename : filename to store highest index written\n",
    "    columns : column names for file\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    begin : starting index (get from index_filename, default 0)\n",
    "    end : index to finish (defaults to end of app_list)\n",
    "    batchsize : number of apps to write in each batch (default 100)\n",
    "    pause : time to wait after each api request (defualt 1)\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    print('Starting at index {}:\\n'.format(begin))\n",
    "    \n",
    "    # by default, process all apps in app_list\n",
    "    if end == -1:\n",
    "        end = len(app_list) + 1\n",
    "    \n",
    "    # generate array of batch begin and end points\n",
    "    batches = np.arange(begin, end, batchsize)\n",
    "    batches = np.append(batches, end)\n",
    "    \n",
    "    apps_written = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches[i+1]\n",
    "        \n",
    "        app_data = get_app_data(start, stop, parser, pause)\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        # writing app data to file\n",
    "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            \n",
    "            for j in range(3,0,-1):\n",
    "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            writer.writerows(app_data)\n",
    "            print('\\rExported lines {}-{} to {}.'.format(start, stop-1, data_filename), end=' ')\n",
    "            \n",
    "        apps_written += len(app_data)\n",
    "        \n",
    "        idx_path = os.path.join(download_path, index_filename)\n",
    "        \n",
    "        # writing last index to file\n",
    "        with open(idx_path, 'w') as f:\n",
    "            index = stop\n",
    "            print(index, file=f)\n",
    "            \n",
    "        # logging time taken\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        batch_times.append(time_taken)\n",
    "        mean_time = statistics.mean(batch_times)\n",
    "        \n",
    "        est_remaining = (len(batches) - i - 2) * mean_time\n",
    "        \n",
    "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
    "        time_td = dt.timedelta(seconds=round(time_taken))\n",
    "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
    "        \n",
    "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
    "            \n",
    "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some functions to handle and prepare the external files.\n",
    "\n",
    "We use `reset_index` for testing and demonstration, allowing us to easily reset the index in the stored file to 0, effectively restarting the entire download process.\n",
    "\n",
    "We define `get_index` to retrieve the index from file, maintaining persistence across sessions. Every time a batch of information (app data) is written to file, we write the highest index within `app_data` that was retrieved. As stated, this is partially for security, ensuring that if there is an error during the download we can read the index from file and continue from the end of the last successful batch. Keeping track of the index also allows us to pause the download, continuing at a later time.\n",
    "\n",
    "Finally, the `prepare_data_file` function readies the csv for storing the data. If the index we retrieved is 0, it means we are either starting for the first time or starting over. In either case, we want a blank csv file with only the header row to begin writing to, se we wipe the file (by opening in write mode) and write the header. Conversely, if the index is anything other than 0, it means we already have downloaded information, and can leave the csv file alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(download_path, index_filename):\n",
    "    \"\"\"Reset index in file to 0.\"\"\"\n",
    "    rel_path = os.path.join(download_path, index_filename)\n",
    "    \n",
    "    with open(rel_path, 'w') as f:\n",
    "        print(0, file=f)\n",
    "        \n",
    "\n",
    "def get_index(download_path, index_filename):\n",
    "    \"\"\"Retrieve index from file, returning 0 if file not found.\"\"\"\n",
    "    try:\n",
    "        rel_path = os.path.join(download_path, index_filename)\n",
    "\n",
    "        with open(rel_path, 'r') as f:\n",
    "            index = int(f.readline())\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        index = 0\n",
    "        \n",
    "    return index\n",
    "\n",
    "\n",
    "def prepare_data_file(download_path, filename, index, columns):\n",
    "    \"\"\"Create file and write headers if index is 0.\"\"\"\n",
    "    if index == 0:\n",
    "        rel_path = os.path.join(download_path, filename)\n",
    "\n",
    "        with open(rel_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Steam Data\n",
    "\n",
    "Now we are ready to start downloading data and writing to file. We define our logic particular to handling the steam API - in fact if no data is returned we return just the name and appid - then begin setting some parameters. We define the files we will write our data and index to, and the columns for the csv file. The API doesn't return every column for every app, so it is best to explicitly set these.\n",
    "\n",
    "Next we run our functions to set up the files, and make a call to `process_batches` to begin the process. Some additional parameters have been added for demonstration, to constrain the download to just a few rows and smaller batches. Removing these would allow the entire download process to be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28230\n",
      "Starting at index 28230:\n",
      "\n",
      "Exported lines 28230-28259 to steam_app_data1.csv. Batch 0 time: 0:00:48 (avg: 0:00:48, remaining: 2:08:23)\n",
      "Exported lines 28260-28289 to steam_app_data1.csv. Batch 1 time: 0:00:49 (avg: 0:00:48, remaining: 2:09:08)\n",
      "Exported lines 28290-28319 to steam_app_data1.csv. Batch 2 time: 0:00:48 (avg: 0:00:48, remaining: 2:07:47)\n",
      "Exported lines 28320-28349 to steam_app_data1.csv. Batch 3 time: 0:00:48 (avg: 0:00:48, remaining: 2:07:04)\n",
      "Exported lines 28350-28379 to steam_app_data1.csv. Batch 4 time: 0:00:47 (avg: 0:00:48, remaining: 2:05:50)\n",
      "Exported lines 28380-28409 to steam_app_data1.csv. Batch 5 time: 0:00:49 (avg: 0:00:48, remaining: 2:05:22)\n",
      "Exported lines 28410-28439 to steam_app_data1.csv. Batch 6 time: 0:00:48 (avg: 0:00:48, remaining: 2:04:40)\n",
      "Exported lines 28440-28469 to steam_app_data1.csv. Batch 7 time: 0:00:48 (avg: 0:00:48, remaining: 2:03:54)\n",
      "Exported lines 28470-28499 to steam_app_data1.csv. Batch 8 time: 0:00:48 (avg: 0:00:48, remaining: 2:03:07)\n",
      "Exported lines 28500-28529 to steam_app_data1.csv. Batch 9 time: 0:00:48 (avg: 0:00:48, remaining: 2:02:24)\n",
      "Exported lines 28530-28559 to steam_app_data1.csv. Batch 10 time: 0:00:48 (avg: 0:00:48, remaining: 2:01:39)\n",
      "Exported lines 28560-28589 to steam_app_data1.csv. Batch 11 time: 0:00:48 (avg: 0:00:48, remaining: 2:00:48)\n",
      "Exported lines 28590-28619 to steam_app_data1.csv. Batch 12 time: 0:00:47 (avg: 0:00:48, remaining: 1:59:52)\n",
      "Exported lines 28620-28649 to steam_app_data1.csv. Batch 13 time: 0:00:48 (avg: 0:00:48, remaining: 1:59:08)\n",
      "Exported lines 28650-28679 to steam_app_data1.csv. Batch 14 time: 0:00:50 (avg: 0:00:48, remaining: 1:58:39)\n",
      "Exported lines 28680-28709 to steam_app_data1.csv. Batch 15 time: 0:00:49 (avg: 0:00:48, remaining: 1:58:01)\n",
      "Exported lines 28710-28739 to steam_app_data1.csv. Batch 16 time: 0:00:50 (avg: 0:00:48, remaining: 1:57:26)\n",
      "Exported lines 28740-28769 to steam_app_data1.csv. Batch 17 time: 0:00:48 (avg: 0:00:48, remaining: 1:56:38)\n",
      "Exported lines 28770-28799 to steam_app_data1.csv. Batch 18 time: 0:00:49 (avg: 0:00:48, remaining: 1:55:53)\n",
      "Current index: 28806\r"
     ]
    }
   ],
   "source": [
    "def parse_steam_request(appid, name):\n",
    "    \"\"\"Unique parser to handle data from Steam Store API.\n",
    "    \n",
    "    Returns : json formatted data (dict-like)\n",
    "    \"\"\"\n",
    "    url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "    parameters = {\"appids\": appid}\n",
    "    \n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    json_app_data = json_data[str(appid)]\n",
    "    \n",
    "    if json_app_data['success']:\n",
    "        data = json_app_data['data']\n",
    "    else:\n",
    "        data = {'name': name, 'steam_appid': appid}\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Set file parameters\n",
    "download_path = './data/download'\n",
    "steam_app_data = 'steam_app_data'+data_id+'.csv'\n",
    "steam_index = 'steam_index'+data_id+'.txt'\n",
    "\n",
    "steam_columns = [\n",
    "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors'\n",
    "]\n",
    "\n",
    "# Overwrites last index for demonstration (would usually store highest index so can continue across sessions)\n",
    "#reset_index(download_path, steam_index)\n",
    "\n",
    "# Retrieve last index downloaded from file\n",
    "index = get_index(download_path, steam_index)\n",
    "print(index)\n",
    "# Wipe or create data file and write headers if index is 0\n",
    "prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
    "\n",
    "# Set end and chunksize for demonstration - remove to run through entire app list\n",
    "process_batches(\n",
    "    parser=parse_steam_request,\n",
    "    app_list=app_list,\n",
    "    download_path=download_path,\n",
    "    data_filename=steam_app_data,\n",
    "    index_filename=steam_index,\n",
    "    columns=steam_columns,\n",
    "    begin=index,\n",
    "    end=end,\n",
    "    batchsize=30, \n",
    "    pause=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect downloaded data\n",
    "raw_steam_data = pd.read_csv('./data/download/steam_app_data'+data_id+'.csv')\n",
    "print(len(raw_steam_data))\n",
    "pd.read_csv('./data/download/steam_app_data'+data_id+'.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download SteamSpy data\n",
    "\n",
    "To retrieve data from SteamSpy we perform a very similar process. Our parse function is a little simpler because of the how data is returned, and the maximum polling rate of this API is higher so we can set a lower value for `pause` in the `process_batches` function and download more quickly. Apart from that we set the new variables and make a call to the `process_batches` function once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steamspy_request(appid, name):\n",
    "    \"\"\"Parser to handle SteamSpy API data.\"\"\"\n",
    "    url = \"https://steamspy.com/api.php\"\n",
    "    parameters = {\"request\": \"appdetails\", \"appid\": appid}\n",
    "    \n",
    "    json_data = get_request(url, parameters)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# set files and columns\n",
    "download_path = './data/download'\n",
    "steamspy_data = 'steamspy_data'+data_id+'.csv'\n",
    "steamspy_index = 'steamspy_index'+data_id+'.txt'\n",
    "\n",
    "steamspy_columns = [\n",
    "    'appid', 'name', 'developer', 'publisher', 'score_rank', 'positive',\n",
    "    'negative', 'userscore', 'owners', 'average_forever', 'average_2weeks',\n",
    "    'median_forever', 'median_2weeks', 'price', 'initialprice', 'discount',\n",
    "    'languages', 'genre', 'ccu', 'tags'\n",
    "]\n",
    "\n",
    "#reset_index(download_path, steamspy_index)\n",
    "index = get_index(download_path, steamspy_index)\n",
    "\n",
    "# Wipe data file if index is 0\n",
    "prepare_data_file(download_path, steamspy_data, index, steamspy_columns)\n",
    "\n",
    "process_batches(\n",
    "    parser=parse_steamspy_request,\n",
    "    app_list=app_list,\n",
    "    download_path=download_path, \n",
    "    data_filename=steamspy_data,\n",
    "    index_filename=steamspy_index,\n",
    "    columns=steamspy_columns,\n",
    "    begin=index,\n",
    "    end=end,\n",
    "    batchsize=20,\n",
    "    pause=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect downloaded steamspy data\n",
    "raw_steamspy_data = pd.read_csv('./data/download/steamspy_data'+data_id+'.csv')\n",
    "raw_steamspy_data.head()\n",
    "pd.read_csv('./data/download/steamspy_data'+data_id+'.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Here we have defined and demonstrated the download process used to generate the data sets.\n",
    "This process already completed can be found here on kaggle [on Kaggle](https://kaggle.com/nikdavis/steam-store-raw) [Only cover games until March 2019].\n",
    "\n",
    "We now have two tables of data with a variety of information about apps on the Steam store. From the Steam data it looks like there are some useful columns like `required_age`, `developers` and `genres` which we can eventually turn into features for analysis, and a `price_overview` column which may inform the success and sales of each game. The `owners` column of the SteamSpy data could be useful, however the [margin of error](https://steamspy.com/about) means data may not be accurate enough for meaningful analysis, we'll have to see what we can manage after cleaning. Instead we may have to use the `positive` and `negative` ratings or average play-time to create our metrics. There is also a `tags` column which appears to crossover with the `categories` and `genres` columns in the Steam data. We may wish to merge these, or keep one over the other.\n",
    "\n",
    "These are all decisions we'll come to in later stages of the project. With the data downloaded, this stage is now complete. In the next step, we'll take care of preparing and cleaning the data, readying a complete data set to use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
